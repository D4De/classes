{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 13:58:22.707532: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-24 13:58:22.732327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 13:58:23.127569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(pretrained_weights=None, input_shape=(256, 256, 1)):\n",
    "    inputs = Input(input_shape)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "\n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/margauxmforsythe/38-cloud-segmentation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://margauxmforsythe/38-cloud-segmentation loaded successfully.\n",
      "\n",
      "HINT: Please forward the port - 46019 to your local machine, if you are running on the cloud.\n",
      " * Serving Flask app 'dataset_visualizer'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"90%\"\n",
       "            height=\"800\"\n",
       "            src=\"https://app.activeloop.ai/visualizer/hub?url=hub://margauxmforsythe/38-cloud-segmentation&token=eyJhbGciOiJIUzUxMiIsImlhdCI6MTY4NzYwODAzMCwiZXhwIjoxNjkxMjA4MDMwfQ.eyJpZCI6InB1YmxpYyJ9.ZjNyfIXpXym6Zn7AjcYHuBr0kJoBWT6IL1R816bemSN7D6ouWhnCI5EUJ0xcunskiXGJ1ztY6Jwev-igRX7Srg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb2d2fb64c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hub\n",
    "\n",
    "ds = hub.dataset(\"hub://margauxmforsythe/38-cloud-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 100\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create folder checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "ds_tf = ds[:image_count].tensorflow()\n",
    "\n",
    "def to_model_fit(item):\n",
    "  x = item['images']\n",
    "  # Normalize\n",
    "  x = x / tf.reduce_max(x)\n",
    "  y = item['masks'] / 255\n",
    "  return (x, y)\n",
    "\n",
    "ds_tf = ds_tf.map(lambda x: to_model_fit(x))\n",
    "\n",
    "train_size = int(0.8 * image_count)\n",
    "val_size = int(0.1 * image_count)\n",
    "test_size = int(0.1 * image_count)\n",
    "\n",
    "\n",
    "ds_tf = ds_tf.shuffle(image_count)\n",
    "test_ds = ds_tf.take(test_size)\n",
    "train_ds = ds_tf.skip(test_size)\n",
    "val_ds = train_ds.take(val_size)\n",
    "train_ds = train_ds.skip(val_size)\n",
    "\n",
    "train_ds = train_ds.shuffle(train_size)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "val_ds = val_ds.shuffle(val_size)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "model = unet(input_shape = (384,384,4))\n",
    "\n",
    "if not os.path.isdir(\"./checkpoints\"):\n",
    "    print(\"Create folder checkpoints\")\n",
    "    os.mkdir(\"./checkpoints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 14:06:14.509135: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/dropout_2/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14/Unknown - 10s 495ms/step - loss: 0.3735 - iou: 0.2413\n",
      "Epoch 1: val_iou improved from -inf to 0.21561, saving model to ./checkpoints/weights.epoch-01-val-iou-0.2156.hdf5\n",
      "14/14 [==============================] - 11s 547ms/step - loss: 0.3735 - iou: 0.2413 - val_loss: 0.3007 - val_iou: 0.2156\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3827 - iou: 0.2508\n",
      "Epoch 2: val_iou improved from 0.21561 to 0.22539, saving model to ./checkpoints/weights.epoch-02-val-iou-0.2254.hdf5\n",
      "14/14 [==============================] - 8s 537ms/step - loss: 0.3827 - iou: 0.2508 - val_loss: 0.2129 - val_iou: 0.2254\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3016 - iou: 0.2399\n",
      "Epoch 3: val_iou improved from 0.22539 to 0.31711, saving model to ./checkpoints/weights.epoch-03-val-iou-0.3171.hdf5\n",
      "14/14 [==============================] - 8s 540ms/step - loss: 0.3016 - iou: 0.2399 - val_loss: 0.2875 - val_iou: 0.3171\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3488 - iou: 0.2370\n",
      "Epoch 4: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 530ms/step - loss: 0.3488 - iou: 0.2370 - val_loss: 0.4347 - val_iou: 0.2158\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2958 - iou: 0.2499\n",
      "Epoch 5: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 533ms/step - loss: 0.2958 - iou: 0.2499 - val_loss: 0.2555 - val_iou: 0.2538\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2926 - iou: 0.2492\n",
      "Epoch 6: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 533ms/step - loss: 0.2926 - iou: 0.2492 - val_loss: 0.2724 - val_iou: 0.2061\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3492 - iou: 0.2690\n",
      "Epoch 7: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 533ms/step - loss: 0.3492 - iou: 0.2690 - val_loss: 0.4051 - val_iou: 0.2400\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3784 - iou: 0.2470\n",
      "Epoch 8: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 533ms/step - loss: 0.3784 - iou: 0.2470 - val_loss: 0.2580 - val_iou: 0.1738\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3798 - iou: 0.2594\n",
      "Epoch 9: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.3798 - iou: 0.2594 - val_loss: 0.3358 - val_iou: 0.2116\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3636 - iou: 0.2583\n",
      "Epoch 10: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.3636 - iou: 0.2583 - val_loss: 0.2680 - val_iou: 0.2267\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2796 - iou: 0.2498\n",
      "Epoch 11: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.2796 - iou: 0.2498 - val_loss: 0.2924 - val_iou: 0.2698\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2629 - iou: 0.2490\n",
      "Epoch 12: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.2629 - iou: 0.2490 - val_loss: 0.2414 - val_iou: 0.1518\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2987 - iou: 0.2450\n",
      "Epoch 13: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.2987 - iou: 0.2450 - val_loss: 0.4091 - val_iou: 0.2972\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3335 - iou: 0.2696\n",
      "Epoch 14: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.3335 - iou: 0.2696 - val_loss: 0.2445 - val_iou: 0.2356\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3434 - iou: 0.2411\n",
      "Epoch 15: val_iou did not improve from 0.31711\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.3434 - iou: 0.2411 - val_loss: 0.2464 - val_iou: 0.2356\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3388 - iou: 0.2410\n",
      "Epoch 16: val_iou improved from 0.31711 to 0.32792, saving model to ./checkpoints/weights.epoch-16-val-iou-0.3279.hdf5\n",
      "14/14 [==============================] - 8s 542ms/step - loss: 0.3388 - iou: 0.2410 - val_loss: 0.2809 - val_iou: 0.3279\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4516 - iou: 0.2599\n",
      "Epoch 17: val_iou improved from 0.32792 to 0.35210, saving model to ./checkpoints/weights.epoch-17-val-iou-0.3521.hdf5\n",
      "14/14 [==============================] - 8s 544ms/step - loss: 0.4516 - iou: 0.2599 - val_loss: 0.3829 - val_iou: 0.3521\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3981 - iou: 0.2600\n",
      "Epoch 18: val_iou did not improve from 0.35210\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.3981 - iou: 0.2600 - val_loss: 0.2078 - val_iou: 0.1815\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3122 - iou: 0.2690\n",
      "Epoch 19: val_iou did not improve from 0.35210\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.3122 - iou: 0.2690 - val_loss: 0.3678 - val_iou: 0.2696\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3139 - iou: 0.2598\n",
      "Epoch 20: val_iou did not improve from 0.35210\n",
      "14/14 [==============================] - 8s 535ms/step - loss: 0.3139 - iou: 0.2598 - val_loss: 0.2702 - val_iou: 0.2560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2d0272b80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('./checkpoints/weights.epoch-{epoch:02d}-val-iou-{val_iou:.4f}.hdf5',\n",
    "                                                 monitor='val_iou', \n",
    "                                                 mode='max', verbose=1,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Recall(name=\"recall\"), \n",
    "                       tf.keras.metrics.Precision(name=\"precision\"), \n",
    "                       tf.keras.metrics.MeanIoU(num_classes=2, name='iou')])\n",
    "\n",
    "model.fit(train_ds, \n",
    "          validation_data=val_ds, \n",
    "          epochs = 20,\n",
    "          callbacks = [checkpoint_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
